{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC515Research Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcoMNhc07rhogB6QEDiSCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lk2255/csc515researchProject/blob/master/CSC515Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGydNRbV6Tw5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysIff2j2IOpr"
      },
      "source": [
        "#Jordanetal2018.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR6j00IBII9y"
      },
      "source": [
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
        "import scipy.stats as st\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import ClassifierMixin\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, Activation\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "\n",
        "import sys\n",
        "import custom_model as cm\n",
        "\n",
        "class LatentHyperNet(BaseEstimator, ClassifierMixin):\n",
        "    __name__ = 'Latent Hyper Net'\n",
        "\n",
        "    def __init__(self, n_iter=1500, eps=1e-6, n_comp=2, mode='regression', dm_method=None, model=None, layers=None):\n",
        "        self.n_iter = n_iter\n",
        "        self.eps = eps\n",
        "        self.n_comp = n_comp\n",
        "        self.mode = mode\n",
        "        self.dm_layer = []\n",
        "        self.dm_method = dm_method\n",
        "        self.model = self.custom_model(model=model, layers=layers)\n",
        "        self.layers = layers\n",
        "\n",
        "    def custom_model(self, model, layers):\n",
        "        input_shape = model.input_shape\n",
        "        input_shape = (input_shape[1], input_shape[2], input_shape[3])\n",
        "        inp = Input(input_shape)\n",
        "        feature_maps = [Model(model.input, model.get_layer(index=i).output)(inp) for i in layers]\n",
        "        model = Model(inp, feature_maps)\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError()\n",
        "\n",
        "        #self.classes_, target = np.unique(y, return_inverse=True)\n",
        "        target = y\n",
        "        target[target == 0] = -1\n",
        "        if self.dm_method == 'lda':\n",
        "            target = np.argmax(target, axis=1)\n",
        "\n",
        "        X = self.extract_features(X)\n",
        "\n",
        "        if self.dm_method == 'pls':\n",
        "            dm = PLSRegression(n_components=self.n_comp, scale=True, max_iter=self.n_iter, tol=self.eps)\n",
        "        elif self.dm_method == 'pca':\n",
        "            dm = PCA(self.n_comp)\n",
        "        elif self.dm_method == 'lda':\n",
        "            dm = LinearDiscriminantAnalysis()\n",
        "\n",
        "        for layer_idx in range(0, len(self.layers)):\n",
        "            dm_ = copy.copy(dm)\n",
        "            dm_.fit(X[layer_idx], target)\n",
        "            self.dm_layer.append(dm_)\n",
        "            del dm_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        import numpy as np\n",
        "        proj_x = None\n",
        "\n",
        "        x = self.extract_features(x)\n",
        "\n",
        "        for layer_idx in range(0, len(self.layers)):\n",
        "            if proj_x is None:\n",
        "                proj_x = self.dm_layer[layer_idx].transform(x[layer_idx])\n",
        "            else:\n",
        "                proj_tmp = self.dm_layer[layer_idx].transform(x[layer_idx])\n",
        "                proj_x = np.column_stack((proj_x, proj_tmp))\n",
        "\n",
        "        return proj_x\n",
        "\n",
        "    def extract_features(self, X, verbose=False):\n",
        "        import time\n",
        "        feat_layers = [[] for x in range(0, len(self.layers))]\n",
        "\n",
        "        idx_sample = 0\n",
        "        for sample in X:\n",
        "            start = time.time()\n",
        "            sample = np.expand_dims(sample, axis=0)\n",
        "            feat = self.model.predict(sample)\n",
        "            for layer in range(0, len(self.layers)):\n",
        "                feat_layers[layer].append(np.reshape(feat[layer], -1))\n",
        "\n",
        "            if verbose == True:\n",
        "                print('Extracting features {}/{} Time[{}]'.format(idx_sample, len(X), time.time() - start))\n",
        "            idx_sample = idx_sample + 1\n",
        "\n",
        "        return feat_layers\n",
        "\n",
        "    def get_features(self, X):\n",
        "        features = self.extract_features(X)\n",
        "        X = None\n",
        "        for layer_idx in range(0, len(self.layers)):\n",
        "            if X is None:\n",
        "                X = features[layer_idx]\n",
        "            else:\n",
        "                X_tmp = features[layer_idx]\n",
        "                X = np.column_stack((X, X_tmp))\n",
        "        X = np.array(X)\n",
        "        return X\n",
        "\n",
        "def custom_model(inp, n_classes, dataset_name):\n",
        "    activation = 'relu'\n",
        "    if dataset_name == 'UTD-MHAD1_1s' or dataset_name =='UTD-MHAD2_1s':\n",
        "        H = Conv2D(filters=24, kernel_size=(12, 2))(inp)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Conv2D(filters=36, kernel_size=(12, 2))(H)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Flatten()(H)\n",
        "        H = Dense(n_classes)(H)\n",
        "        H = Activation('softmax')(H)\n",
        "\n",
        "        model = Model([inp], H)\n",
        "    else:\n",
        "        H = Conv2D(filters=24, kernel_size=(12, 1))(inp)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Conv2D(filters=32, kernel_size=(12, 1))(H)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Conv2D(filters=40, kernel_size=(6, 1))(H)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Conv2D(filters=48, kernel_size=(2, 1))(H)\n",
        "        H = Activation(activation)(H)\n",
        "        H = MaxPooling2D(pool_size=(2, 1))(H)\n",
        "\n",
        "        H = Flatten()(H)\n",
        "        H = Dense(n_classes)(H)\n",
        "        H = Activation('softmax')(H)\n",
        "\n",
        "        model = Model([inp], H)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Paper: Latent HyperNet: Exploring the Layers of Convolutional Neural Networks\n",
        "    np.random.seed(12227)\n",
        "\n",
        "    if (len(sys.argv) > 1):\n",
        "        data_input_file = sys.argv[1]\n",
        "    else:\n",
        "        data_input_file = 'data/LOSO/MHEALTH.npz'\n",
        "\n",
        "    dataset_name = data_input_file.split('/')\n",
        "    dataset_name = dataset_name[-1].replace('.npz', '')\n",
        "\n",
        "    if dataset_name == 'UTD-MHAD1_1s' or dataset_name == 'UTD-MHAD2_1s':\n",
        "        layers = [3, 6]\n",
        "    else:\n",
        "        layers = [3, 6, 9]\n",
        "\n",
        "    tmp = np.load(data_input_file)\n",
        "    X = tmp['X']\n",
        "    y = tmp['y']\n",
        "    folds = tmp['folds']\n",
        "\n",
        "    n_class = y.shape[1]\n",
        "    _, _, img_rows, img_cols = X.shape\n",
        "    avg_acc = []\n",
        "    avg_recall = []\n",
        "    avg_f1 = []\n",
        "\n",
        "    print('Jordao et al. 2018 {}'.format(data_input_file))\n",
        "    for i in range(0, len(folds)):\n",
        "        train_idx = folds[i][0]\n",
        "        test_idx = folds[i][1]\n",
        "\n",
        "        X_train = X[train_idx]\n",
        "        X_test = X[test_idx]\n",
        "\n",
        "        inp = Input((1, img_rows, img_cols))\n",
        "        model = custom_model(inp, n_classes=n_class, dataset_name=dataset_name)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adadelta')\n",
        "        model.fit(X_train, y[train_idx], batch_size=cm.bs, epochs=cm.n_ep,\n",
        "                  verbose=0, callbacks=[cm.custom_stopping(value=cm.loss, verbose=2)],\n",
        "                  validation_data=(X_train, y[train_idx]))\n",
        "\n",
        "\n",
        "        hyper_net = LatentHyperNet(n_comp=19, model=model, layers=layers, dm_method='pls')\n",
        "        hyper_net.fit(X_train, y[train_idx])\n",
        "        X_train = hyper_net.transform(X_train)\n",
        "        X_test = hyper_net.transform(X_test)\n",
        "\n",
        "        inp = Input((X_train.shape[1],))\n",
        "        fc = Dense(n_class)(inp)\n",
        "        model = Activation('softmax')(fc)\n",
        "        model = Model(inp, model)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adadelta')\n",
        "        callbacks = [cm.custom_stopping(value=cm.loss, verbose=2)]\n",
        "\n",
        "        model.fit(X_train, y[train_idx], batch_size=len(X_train),\n",
        "                  epochs=4*cm.n_ep,#The drawback of the method is that it requires more iterations to converge (loss <= cm.loss)\n",
        "                   verbose=0, callbacks=callbacks, validation_data=(X_train, y[train_idx]))\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        y_true = np.argmax(y[test_idx], axis=1)\n",
        "\n",
        "        acc_fold = accuracy_score(y_true, y_pred)\n",
        "        avg_acc.append(acc_fold)\n",
        "\n",
        "        recall_fold = recall_score(y_true, y_pred, average='macro')\n",
        "        avg_recall.append(recall_fold)\n",
        "\n",
        "        f1_fold = f1_score(y_true, y_pred, average='macro')\n",
        "        avg_f1.append(f1_fold)\n",
        "\n",
        "        print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold, i))\n",
        "        print('______________________________________________________')\n",
        "        del model\n",
        "\n",
        "    ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
        "    ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
        "    ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc=np.mean(avg_f1), scale=st.sem(avg_f1))\n",
        "    print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
        "    print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
        "    print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z74bGpgrIZoI"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7jcG3g3IjGH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGUoA7YAGeH"
      },
      "source": [
        "#Paper\n",
        "[linked paper: ](https://arxiv.org/pdf/1806.05226v3.pdf"
      ]
    }
  ]
}